{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cit1CALmDobv"
      },
      "source": [
        "# Corrective RAG\n",
        "\n",
        "![corrective rag](https://pbs.twimg.com/media/GF1JIZ_aEAE1cZ1.jpg:large \"100x100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTUimETK5EY_",
        "outputId": "edc043a1-94c0-4299-a178-3e5b46bea6bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=22.04\n",
            "DISTRIB_CODENAME=jammy\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\n",
            "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ],
      "source": [
        "!cat /etc/*-release"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDJCf2rw6Qxk"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCYVIBvj6NcN"
      },
      "source": [
        "## Install python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzDKJKGo6MbU"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python langchain-mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcUvKAWE5y0L"
      },
      "source": [
        "## ![](https://ollama.com/public/ollama.png) Download and Install ollama\n",
        "\n",
        "\n",
        "\n",
        "Visit [Ollama](https://ollama.com/) to dowload it manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCln6CfmgSuC",
        "outputId": "d6843589-243a-4ca2-96ed-d34d36a3e2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA GPU. Install lspci or lshw to automatically detect and install NVIDIA CUDA drivers.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCPMo-TCmnE1"
      },
      "outputs": [],
      "source": [
        "!ollama serve & ollama pull mistral:instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7veFj_iF6CKT"
      },
      "source": [
        "## Set Environment secret variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQmKJ6FQ6AN_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_API_KEY'] = 'ls__91f5ff25d23a450990347fa2aeaf159f'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['TAVILY_API_KEY'] = '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'\n",
        "mistral_api_key = \"<Mistral-Key>\" # if using"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VZQCGdC6F7B"
      },
      "source": [
        "## Set model and other configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSkqDoALm8uT"
      },
      "outputs": [],
      "source": [
        "run_local = \"Yes\"\n",
        "local_llm = \"mistral:instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4syB_nb7OAv"
      },
      "source": [
        "## Setup Llama.cpp (Optional if using GPT4All)\n",
        "\n",
        "\n",
        "GPT4All embeddings are much better than Llama.cpp embeddings as GPT4All embeddings retrieval are more richer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emQ7lrPLoz9y",
        "outputId": "93c3789c-298e-4061-8653-bd540a8e7751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 19513, done.\u001b[K\n",
            "remote: Counting objects: 100% (11195/11195), done.\u001b[K\n",
            "remote: Compressing objects: 100% (543/543), done.\u001b[K\n",
            "remote: Total 19513 (delta 10811), reused 10720 (delta 10651), pack-reused 8318\u001b[K\n",
            "Receiving objects: 100% (19513/19513), 19.90 MiB | 22.72 MiB/s, done.\n",
            "Resolving deltas: 100% (13938/13938), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhwsY-3Wo7AH",
        "outputId": "febf934d-e69c-4d1b-e67e-186a9fda044f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llama.cpp\n",
            "I ccache not found. Consider installing it for faster compilation.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG \n",
            "I NVCCFLAGS: -std=c++11 -O3 \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c llama.cpp -o llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/common.cpp -o common.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/sampling.cpp -o sampling.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/build-info.cpp -o build-info.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/console.cpp -o console.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/main/main.cpp -o examples/main/main.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/main/main.o -o main  \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize/quantize.o -o quantize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/quantize-stats/quantize-stats.o -o quantize-stats  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/perplexity/perplexity.o -o perplexity  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/imatrix/imatrix.o -o imatrix  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/embedding/embedding.o -o embedding  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/vdot.o -o vdot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o pocs/vdot/q8dot.o -o q8dot  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c common/train.cpp -o train.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/simple/simple.o -o simple  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched/batched.o -o batched  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o llama.o common.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/batched-bench/batched-bench.o -o batched-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/save-load-state/save-load-state.o -o save-load-state  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/server/server.cpp -o examples/server/server.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/clip.cpp -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -Iexamples/server examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/server/server.o examples/llava/clip.o -o server   \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/gguf/gguf.o -o gguf  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llama-bench/llama-bench.o -o llama-bench  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/baby-llama/baby-llama.o -o baby-llama  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/beam-search/beam-search.o -o beam-search  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/speculative/speculative.o -o speculative  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/infill/infill.o -o infill  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/tokenize/tokenize.o -o tokenize  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  build-info.o ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/parallel/parallel.o -o parallel  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/finetune/finetune.o -o finetune  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/export-lora/export-lora.o -o export-lora  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookahead/lookahead.o -o lookahead  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/lookup/lookup.o -o lookup  \n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
            "g++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-alloc.o ggml-backend.o ggml-quants.o examples/passkey/passkey.o -o passkey  \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd llama.cpp\n",
        "!make\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbF3lUCX8eoE"
      },
      "source": [
        "### Download embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISz6PZYy8dyV",
        "outputId": "f406f9ac-815f-42c8-9c61-d67757f1a3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-01 06:12:59--  https://huggingface.co/nomic-ai/nomic-embed-text-v1-GGUF/resolve/main/nomic-embed-text-v1.Q2_K.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.4, 18.172.134.88, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/a6/63/a663fb46c8f5bfbd779f9398e78efdd80ccde64ee2834e1b61ff49a34748575d/afb87e81c67d34db721db27f093d1e87e4620001fe11e4566c5ceb88cd0fc667?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27nomic-embed-text-v1.Q2_K.gguf%3B+filename%3D%22nomic-embed-text-v1.Q2_K.gguf%22%3B&Expires=1709532779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTUzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E2LzYzL2E2NjNmYjQ2YzhmNWJmYmQ3NzlmOTM5OGU3OGVmZGQ4MGNjZGU2NGVlMjgzNGUxYjYxZmY0OWEzNDc0ODU3NWQvYWZiODdlODFjNjdkMzRkYjcyMWRiMjdmMDkzZDFlODdlNDYyMDAwMWZlMTFlNDU2NmM1Y2ViODhjZDBmYzY2Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Sekhn4OfBLPRybq9uH7OMk4RU0EHsKu8bBYJi1BSjdxbL0jjhl-dztI4DNcEkKt40jOnVydzy2VUImxS0aB8wF6CO5XojkVwMCZsxuumEpg4khqX8XmClE56i4JVLVTSbAwNKUY4c6tOdCHBMi6Jtjwhfr0Ac1gcvkZAFxR4tofcyogkyTiV8boPvbohvZpUrFRbKPLlYPblviObYdnqUmEx6HpSctd48B1iacVkCwWIN9ymX%7EjRyiwjH8Ez9X5JJiWs8PLV7SLvludEkeeX6Aqpu%7EM1v9FspzfrJUIB4IGYkO8bjEqGFgpWp33zBmTJ%7E94eT5tD24TFOnjsPJQnOw__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2024-03-01 06:12:59--  https://cdn-lfs-us-1.huggingface.co/repos/a6/63/a663fb46c8f5bfbd779f9398e78efdd80ccde64ee2834e1b61ff49a34748575d/afb87e81c67d34db721db27f093d1e87e4620001fe11e4566c5ceb88cd0fc667?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27nomic-embed-text-v1.Q2_K.gguf%3B+filename%3D%22nomic-embed-text-v1.Q2_K.gguf%22%3B&Expires=1709532779&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTUzMjc3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2E2LzYzL2E2NjNmYjQ2YzhmNWJmYmQ3NzlmOTM5OGU3OGVmZGQ4MGNjZGU2NGVlMjgzNGUxYjYxZmY0OWEzNDc0ODU3NWQvYWZiODdlODFjNjdkMzRkYjcyMWRiMjdmMDkzZDFlODdlNDYyMDAwMWZlMTFlNDU2NmM1Y2ViODhjZDBmYzY2Nz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Sekhn4OfBLPRybq9uH7OMk4RU0EHsKu8bBYJi1BSjdxbL0jjhl-dztI4DNcEkKt40jOnVydzy2VUImxS0aB8wF6CO5XojkVwMCZsxuumEpg4khqX8XmClE56i4JVLVTSbAwNKUY4c6tOdCHBMi6Jtjwhfr0Ac1gcvkZAFxR4tofcyogkyTiV8boPvbohvZpUrFRbKPLlYPblviObYdnqUmEx6HpSctd48B1iacVkCwWIN9ymX%7EjRyiwjH8Ez9X5JJiWs8PLV7SLvludEkeeX6Aqpu%7EM1v9FspzfrJUIB4IGYkO8bjEqGFgpWp33zBmTJ%7E94eT5tD24TFOnjsPJQnOw__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 108.156.107.44, 108.156.107.80, 108.156.107.29, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|108.156.107.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49361088 (47M) [application/octet-stream]\n",
            "Saving to: ‘llama.cpp/model/nomic-embed-text-v1.Q2_K.gguf’\n",
            "\n",
            "nomic-embed-text-v1 100%[===================>]  47.07M  70.7MB/s    in 0.7s    \n",
            "\n",
            "2024-03-01 06:13:00 (70.7 MB/s) - ‘llama.cpp/model/nomic-embed-text-v1.Q2_K.gguf’ saved [49361088/49361088]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/nomic-ai/nomic-embed-text-v1-GGUF/resolve/main/nomic-embed-text-v1.Q2_K.gguf -P llama.cpp/model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aiItL378qeV",
        "outputId": "bbcd93f5-b0bb-4c94-af40-255564ffe5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nomic-embed-text-v1.Q2_K.gguf\n"
          ]
        }
      ],
      "source": [
        "!ls llama.cpp/model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEtbrM_M8mXo"
      },
      "source": [
        "### Install llama cpp python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5qPv-u08JEu",
        "outputId": "2ba96338-d2e6-4f7e-fcb7-bc640fa4f298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.53.tar.gz (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.53-cp310-cp310-manylinux_2_35_x86_64.whl size=2751939 sha256=f2ab2178b8ca803c67b8e7dffc787894b20b7bdb7984c0c719c4b742fe90887b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/94/10/5407d0fd0a979d77bef8df3d298af7d1d60193b6b759ad217f\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.53\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqCyaMyd8J-w"
      },
      "source": [
        "## Install GPT4ALL (if not using Llama.cpp)\n",
        "\n",
        "\n",
        "> For using GPT4ALL Embeddings ubuntu-22 is required otherwise use LlamaCPP embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Ef-yFt2Ltq",
        "outputId": "9007f743-49be-4301-fd55-7e9edc64d146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.2.1.post1-py3-none-manylinux1_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt4all) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt4all) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt4all) (2024.2.2)\n",
            "Installing collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.2.1.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rTeaA7V8Q1w"
      },
      "source": [
        "# Indexing\n",
        "\n",
        "## Split documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tLvt_BDnL30",
        "outputId": "9b5c8704-ed52-4efa-d728-d6851c4e854c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total splits created: 32\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Load url\n",
        "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "\n",
        "# split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size= 500,\n",
        "    chunk_overlap= 100\n",
        ")\n",
        "all_splits= text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Total splits created: {len(all_splits)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2gkHG_Nn_RT",
        "outputId": "82da8136-9cb2-45c8-e7a4-4f64e39c4ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lowest split size: 48\n",
            "(25, 'Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', 48)\n"
          ]
        }
      ],
      "source": [
        "# Lowest chunk in all_splits\n",
        "print(f\"Lowest split size: {min(len(d.page_content) for d in all_splits)}\")\n",
        "min_value = sorted([(i, d.page_content, len(d.page_content)) for i, d in enumerate(all_splits)],key= lambda x: x[2])[0]\n",
        "print(min_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ieckevdb8teE"
      },
      "source": [
        "## Create retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSgVWCq_oav7",
        "outputId": "f59aa32a-8e1b-49fb-fa48-b221eb4fbda5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 45.9M/45.9M [00:00<00:00, 135MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import LlamaCppEmbeddings, GPT4AllEmbeddings\n",
        "\n",
        "# Embed and index\n",
        "if run_local == \"Yes\":\n",
        "    # GPT4All\n",
        "    embedding = GPT4AllEmbeddings()\n",
        "    # embd_model_path = os.path.join(os.getcwd(), \"llama.cpp\", \"model\", \"nomic-embed-text-v1.Q2_K.gguf\")\n",
        "    # embedding = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
        "else:\n",
        "    pass\n",
        "\n",
        "# Index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents= all_splits,\n",
        "    collection_name= \"rag-chroma-1.0\",\n",
        "    embedding= embedding,\n",
        ")\n",
        "\n",
        "retriever= vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAQhYmltmxFz"
      },
      "source": [
        "## Llama.cpp retriever (optional)\n",
        "\n",
        "This retriever is optional as GPT4ALL embeddings are better than Llama.cpp embeddings. This can also be checked in the next section where we use \"Agent memory\" as query for both retrievers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWyYf5y1nJiU",
        "outputId": "0411cdb6-9740-48b4-c696-4ec8c489408b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 112 tensors from /content/llama.cpp/model/nomic-embed-text-v1.Q2_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert\n",
            "llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1\n",
            "llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12\n",
            "llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048\n",
            "llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768\n",
            "llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072\n",
            "llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000\n",
            "llama_model_loader: - kv   8:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false\n",
            "llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1\n",
            "llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000\n",
            "llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2\n",
            "llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101\n",
            "llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100\n",
            "llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   51 tensors\n",
            "llama_model_loader: - type q2_K:   37 tensors\n",
            "llama_model_loader: - type q3_K:   24 tensors\n",
            "llm_load_vocab: mismatch in special tokens definition ( 7104/30522 vs 5/30522 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = nomic-bert\n",
            "llm_load_print_meta: vocab type       = WPM\n",
            "llm_load_print_meta: n_vocab          = 30522\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 2048\n",
            "llm_load_print_meta: n_embd           = 768\n",
            "llm_load_print_meta: n_head           = 12\n",
            "llm_load_print_meta: n_head_kv        = 12\n",
            "llm_load_print_meta: n_layer          = 12\n",
            "llm_load_print_meta: n_rot            = 64\n",
            "llm_load_print_meta: n_embd_head_k    = 64\n",
            "llm_load_print_meta: n_embd_head_v    = 64\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 768\n",
            "llm_load_print_meta: n_embd_v_gqa     = 768\n",
            "llm_load_print_meta: f_norm_eps       = 1.0e-12\n",
            "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 3072\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 1\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 137M\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 136.73 M\n",
            "llm_load_print_meta: model size       = 46.35 MiB (2.84 BPW) \n",
            "llm_load_print_meta: general.name     = nomic-embed-text-v1\n",
            "llm_load_print_meta: BOS token        = 101 '[CLS]'\n",
            "llm_load_print_meta: EOS token        = 102 '[SEP]'\n",
            "llm_load_print_meta: UNK token        = 100 '[UNK]'\n",
            "llm_load_print_meta: SEP token        = 102 '[SEP]'\n",
            "llm_load_print_meta: PAD token        = 0 '[PAD]'\n",
            "llm_load_tensors: ggml ctx size =    0.04 MiB\n",
            "llm_load_tensors:        CPU buffer size =    46.35 MiB\n",
            "..................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 1000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    18.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   18.00 MiB, K (f16):    9.00 MiB, V (f16):    9.00 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =     3.51 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =    21.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'tokenizer.ggml.bos_token_id': '101', 'general.architecture': 'nomic-bert', 'general.name': 'nomic-embed-text-v1', 'nomic-bert.block_count': '12', 'nomic-bert.pooling_type': '1', 'nomic-bert.attention.head_count': '12', 'tokenizer.ggml.token_type_count': '2', 'nomic-bert.context_length': '2048', 'nomic-bert.rope.freq_base': '1000.000000', 'nomic-bert.embedding_length': '768', 'nomic-bert.feed_forward_length': '3072', 'nomic-bert.attention.layer_norm_epsilon': '0.000000', 'general.file_type': '10', 'nomic-bert.attention.causal': 'false'}\n"
          ]
        }
      ],
      "source": [
        "embd_model_path = os.path.join(os.getcwd(), \"llama.cpp\", \"model\", \"nomic-embed-text-v1.Q2_K.gguf\")\n",
        "llama_embedding = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
        "\n",
        "# Index\n",
        "llama_vectorstore = Chroma.from_documents(\n",
        "    documents= all_splits,\n",
        "    collection_name= \"rag-chroma-llama\",\n",
        "    embedding= embedding,\n",
        ")\n",
        "\n",
        "llama_retriever= llama_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phSB3Jh28zsh"
      },
      "source": [
        "## Retrieve example document to check if collection has data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGCIOgK93oMi",
        "outputId": "d805bed2-3155-4afa-ebfc-085306450b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
            "\n",
            "Each element is an observation, an event directly provided by the agent.\n",
            "- Inter-agent communication can trigger new natural language statements.\n",
            "\n",
            "\n",
            "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
            "\n",
            "Recency: recent events have higher scores\n",
            "Importance: distinguish mundane from core memories. Ask LM directly.\n",
            "Relevance: based on how related it is to the current situation / query.\n",
            "\n",
            "\n",
            "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
            "\n",
            "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
            "\n",
            "\n",
            "Planning & Reacting: translate the reflections and the environment information into actions\n",
            "\n",
            "Planning is essentially in order to optimize believability at the moment vs in time.\n",
            "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n",
            "========================================\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FAQ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "emojisearch.app\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\n",
            "\n",
            "Agent System Overview\n",
            "\n",
            "Component One: Planning\n",
            "\n",
            "Task Decomposition\n",
            "\n",
            "Self-Reflection\n",
            "\n",
            "\n",
            "Component Two: Memory\n",
            "\n",
            "Types of Memory\n",
            "\n",
            "Maximum Inner Product Search (MIPS)\n",
            "\n",
            "\n",
            "Component Three: Tool Use\n",
            "\n",
            "Case Studies\n",
            "\n",
            "Scientific Discovery Agent\n",
            "\n",
            "Generative Agents Simulation\n",
            "\n",
            "Proof-of-Concept Examples\n",
            "\n",
            "\n",
            "Challenges\n",
            "\n",
            "Citation\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
            "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "\n",
            "Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
            "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
            "\n",
            "\n",
            "Tool use\n",
            "========================================\n",
            "Planning & Reacting: translate the reflections and the environment information into actions\n",
            "\n",
            "Planning is essentially in order to optimize believability at the moment vs in time.\n",
            "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
            "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
            "Proof-of-Concept Examples#\n",
            "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
            "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
            "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
            "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. {{user-provided goal 1}}\n",
            "2. {{user-provided goal 2}}\n",
            "3. ...\n",
            "4. ...\n",
            "5. ...\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "5. Use subprocesses for commands that will not terminate within a few minutes\n",
            "========================================\n",
            "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
            "\n",
            "\n",
            "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
            "\n",
            "\n",
            "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
            "\n",
            "\n",
            "Citation#\n",
            "Cited as:\n",
            "\n",
            "Weng, Lilian. (Jun 2023). LLM-powered Autonomous Agents\". Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n"
          ]
        }
      ],
      "source": [
        "for d in retriever.get_relevant_documents(\"Agent memory\"):\n",
        "    print(\"=\" * 40)\n",
        "    print(d.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgevQ4lEpDkC"
      },
      "source": [
        "## Llama | Retrieve example document to check if collection has data (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQvQyBq9pJ_N",
        "outputId": "c6c4e79b-1e07-4f94-fd5f-ccda00f5ac39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================\n",
            "Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\n",
            "\n",
            "Each element is an observation, an event directly provided by the agent.\n",
            "- Inter-agent communication can trigger new natural language statements.\n",
            "\n",
            "\n",
            "Retrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\n",
            "\n",
            "Recency: recent events have higher scores\n",
            "Importance: distinguish mundane from core memories. Ask LM directly.\n",
            "Relevance: based on how related it is to the current situation / query.\n",
            "\n",
            "\n",
            "Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
            "\n",
            "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
            "\n",
            "\n",
            "Planning & Reacting: translate the reflections and the environment information into actions\n",
            "\n",
            "Planning is essentially in order to optimize believability at the moment vs in time.\n",
            "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n",
            "========================================\n",
            "LLM Powered Autonomous Agents | Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Lil'Log\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Posts\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Archive\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Search\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tags\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FAQ\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "emojisearch.app\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\n",
            "\n",
            "Agent System Overview\n",
            "\n",
            "Component One: Planning\n",
            "\n",
            "Task Decomposition\n",
            "\n",
            "Self-Reflection\n",
            "\n",
            "\n",
            "Component Two: Memory\n",
            "\n",
            "Types of Memory\n",
            "\n",
            "Maximum Inner Product Search (MIPS)\n",
            "\n",
            "\n",
            "Component Three: Tool Use\n",
            "\n",
            "Case Studies\n",
            "\n",
            "Scientific Discovery Agent\n",
            "\n",
            "Generative Agents Simulation\n",
            "\n",
            "Proof-of-Concept Examples\n",
            "\n",
            "\n",
            "Challenges\n",
            "\n",
            "Citation\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
            "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "\n",
            "Memory\n",
            "\n",
            "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
            "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
            "\n",
            "\n",
            "Tool use\n",
            "========================================\n",
            "Planning & Reacting: translate the reflections and the environment information into actions\n",
            "\n",
            "Planning is essentially in order to optimize believability at the moment vs in time.\n",
            "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
            "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
            "Environment information is present in a tree structure.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
            "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
            "Proof-of-Concept Examples#\n",
            "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
            "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
            "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
            "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
            "\n",
            "GOALS:\n",
            "\n",
            "1. {{user-provided goal 1}}\n",
            "2. {{user-provided goal 2}}\n",
            "3. ...\n",
            "4. ...\n",
            "5. ...\n",
            "\n",
            "Constraints:\n",
            "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
            "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
            "3. No user assistance\n",
            "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
            "5. Use subprocesses for commands that will not terminate within a few minutes\n",
            "========================================\n",
            "Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\n",
            "\n",
            "\n",
            "Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\n",
            "\n",
            "\n",
            "Reliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\n",
            "\n",
            "\n",
            "Citation#\n",
            "Cited as:\n",
            "\n",
            "Weng, Lilian. (Jun 2023). LLM-powered Autonomous Agents\". Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\n"
          ]
        }
      ],
      "source": [
        "for d in llama_retriever.get_relevant_documents(\"Agent memory\"):\n",
        "    print(\"=\" * 40)\n",
        "    print(d.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIxkxMSD88kw"
      },
      "source": [
        "# Start async process to start `ollama serve`\n",
        "\n",
        "This will start the `ollama serve` command which is necessary to run `ollama run mistral:instruct`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xIiOexitK8e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# NB: You may need to set these depending and get cuda working depending which backend you are running.\n",
        "# Set environment variable for NVIDIA library\n",
        "# Set environment variables for CUDA\n",
        "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
        "# Set LD_LIBRARY_PATH to include both /usr/lib64-nvidia and CUDA lib directories\n",
        "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
        "\n",
        "async def run_process(cmd):\n",
        "    print('>>> starting', *cmd)\n",
        "    process = await asyncio.create_subprocess_exec(\n",
        "        *cmd,\n",
        "        stdout=asyncio.subprocess.PIPE,\n",
        "        stderr=asyncio.subprocess.PIPE\n",
        "    )\n",
        "\n",
        "    # define an async pipe function\n",
        "    async def pipe(lines):\n",
        "        async for line in lines:\n",
        "            print(line.decode().strip())\n",
        "\n",
        "        await asyncio.gather(\n",
        "            pipe(process.stdout),\n",
        "            pipe(process.stderr),\n",
        "        )\n",
        "\n",
        "    # call it\n",
        "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-USRF2h0UZr",
        "outputId": "c397fc83-b512-476e-8b91-e430d5b55979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting ollama serve\n",
            "time=2024-03-01T09:53:45.885Z level=INFO source=images.go:710 msg=\"total blobs: 5\"\n",
            "time=2024-03-01T09:53:45.885Z level=INFO source=images.go:717 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-03-01T09:53:45.886Z level=INFO source=routes.go:1019 msg=\"Listening on 127.0.0.1:11434 (version 0.1.27)\"\n",
            "time=2024-03-01T09:53:45.886Z level=INFO source=payload_common.go:107 msg=\"Extracting dynamic libraries...\"\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import threading\n",
        "\n",
        "async def start_ollama_serve():\n",
        "    await run_process(['ollama', 'serve'])\n",
        "\n",
        "def run_async_in_thread(loop, coro):\n",
        "    asyncio.set_event_loop(loop)\n",
        "    loop.run_until_complete(coro)\n",
        "    loop.close()\n",
        "\n",
        "# Create a new event loop that will run in a new thread\n",
        "new_loop = asyncio.new_event_loop()\n",
        "\n",
        "# Start ollama serve in a separate thread so the cell won't block execution\n",
        "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69B1GLSL9O4B"
      },
      "source": [
        "# Simple chain to use retriever\n",
        "\n",
        "- This chain will take a promt template to instruct llm that it is a grader and need to output json as its response.\n",
        "\n",
        "- The json output will then pass through a JsonOutputParser which will parse the string json to python native dict format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e5urm3BqflQ",
        "outputId": "722d93fa-ecf5-456e-f04b-1d59aadc8b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time=2024-03-01T06:16:44.280Z level=INFO source=payload_common.go:146 msg=\"Dynamic LLM libraries [rocm_v6 rocm_v5 cpu cpu_avx cpu_avx2 cuda_v11]\"\n",
            "time=2024-03-01T06:16:44.280Z level=INFO source=gpu.go:94 msg=\"Detecting GPU type\"\n",
            "time=2024-03-01T06:16:44.280Z level=INFO source=gpu.go:265 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-03-01T06:16:44.285Z level=INFO source=gpu.go:311 msg=\"Discovered GPU libraries: [/usr/lib64-nvidia/libnvidia-ml.so.535.104.05]\"\n",
            "time=2024-03-01T06:16:44.294Z level=INFO source=gpu.go:99 msg=\"Nvidia GPU detected\"\n",
            "time=2024-03-01T06:16:44.294Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-03-01T06:16:44.300Z level=INFO source=gpu.go:146 msg=\"CUDA Compute Capability detected: 7.5\"\n",
            "time=2024-03-01T06:16:44.601Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-03-01T06:16:44.602Z level=INFO source=gpu.go:146 msg=\"CUDA Compute Capability detected: 7.5\"\n",
            "time=2024-03-01T06:16:44.602Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-03-01T06:16:44.602Z level=INFO source=gpu.go:146 msg=\"CUDA Compute Capability detected: 7.5\"\n",
            "time=2024-03-01T06:16:44.602Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-03-01T06:16:44.614Z level=INFO source=dyn_ext_server.go:90 msg=\"Loading Dynamic llm server: /tmp/ollama3838008902/cuda_v11/libext_server.so\"\n",
            "time=2024-03-01T06:16:44.614Z level=INFO source=dyn_ext_server.go:150 msg=\"Initializing llama server\"\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   yes\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: no\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256:e8a35b5937a5e6d5c35d1f2a15f161e07eefe5e5bb0a3cdd42998ee79b057730 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = [\"▁ t\", \"i n\", \"e r\", \"▁ a\", \"h e...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_0\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW)\n",
            "llm_load_print_meta: general.name     = mistralai\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  3847.55 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =    13.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   164.01 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.00 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 3\n",
            "time=2024-03-01T06:16:46.996Z level=INFO source=dyn_ext_server.go:161 msg=\"Starting llama main loop\"\n",
            "[GIN] 2024/03/01 - 06:16:48 | 200 |  3.721443036s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "{'score': 'yes'}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# LLM\n",
        "llm= ChatOllama(model= local_llm, format=\"json\", temperature= 0)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "    Here is the user question: {question} \\n\n",
        "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
        "    input_variables=[\"question\", \"context\"],\n",
        ")\n",
        "\n",
        "chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "question = \"Explain how the different types of agent memory work?\"\n",
        "docs= retriever.get_relevant_documents(question)\n",
        "score = chain.invoke({\"question\": question, \"context\": docs[0].page_content})\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWtNAMHv9pyJ"
      },
      "source": [
        "# State\n",
        "\n",
        "Every node in our graph will modify `state`, which is dict that contains values (`question`, `documents`, etc) relevant to RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmv3AN4jr5NU"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Dict, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        keys: A dictionary where each key is a string.\n",
        "    \"\"\"\n",
        "\n",
        "    keys: Dict[str, any]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_3NvSPs93u9"
      },
      "source": [
        "# Nodes and Edges\n",
        "\n",
        "Every node in the graph we laid out above is a function.\n",
        "\n",
        "Each node will modify the state in some way.\n",
        "\n",
        "Each edge will choose which node to call next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F74BANTt1PEk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "### Nodes ###\n",
        "\n",
        "\n",
        "def dprint(*args)->None:\n",
        "    for txt in args:\n",
        "        print(\"-\" * 20 + f\" {txt} \" + \"-\" * 20)\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    local = state_dict[\"local\"]\n",
        "    retriever_to_use = state_dict['retriever_to_use']\n",
        "    dprint(retriever_to_use, \"GPT4All using= \" + str(retriever_to_use == \"GPT4All\"))\n",
        "    documents = retriever.get_relevant_documents(question) if retriever_to_use == \"GPT4All\" else llama_retriever.get_relevant_documents(question)\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # Prompt\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    # LLM\n",
        "    if local == \"Yes\":\n",
        "        llm = ChatOllama(model=local_llm, temperature=0)\n",
        "    else:\n",
        "        llm = ChatMistralAI(\n",
        "            model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
        "        )\n",
        "\n",
        "    # Post-processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Run\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "    }\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK RELEVANCE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # LLM\n",
        "    if local == \"Yes\":\n",
        "        llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "    else:\n",
        "        llm = ChatMistralAI(\n",
        "            mistral_api_key=mistral_api_key, temperature=0, model=\"mistral-medium\"\n",
        "        )\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
        "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "    # Score\n",
        "    filtered_docs = []\n",
        "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
        "    for d in documents:\n",
        "        score = chain.invoke(\n",
        "            {\n",
        "                \"question\": question,\n",
        "                \"context\": d.page_content,\n",
        "            }\n",
        "        )\n",
        "        grade = score[\"score\"]\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            search = \"Yes\"  # Perform web search\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        \"keys\": {\n",
        "            \"documents\": filtered_docs,\n",
        "            \"question\": question,\n",
        "            \"local\": local,\n",
        "            \"run_web_search\": search,\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # Create a prompt template with format instructions and the query\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n\n",
        "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n\n",
        "        Here is the initial question:\n",
        "        \\n ------- \\n\n",
        "        {question}\n",
        "        \\n ------- \\n\n",
        "        Provide an improved question without any premable, only respond with the updated question: \"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "\n",
        "    # Grader\n",
        "    # LLM\n",
        "    if local == \"Yes\":\n",
        "        llm = ChatOllama(model=local_llm, temperature=0)\n",
        "    else:\n",
        "        llm = ChatMistralAI(\n",
        "            mistral_api_key=mistral_api_key, temperature=0, model=\"mistral-medium\"\n",
        "        )\n",
        "\n",
        "    # Prompt\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    better_question = chain.invoke({\"question\": question})\n",
        "\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": better_question, \"local\": local}\n",
        "    }\n",
        "\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question using Tavily API.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Web results appended to documents.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    tool = TavilySearchResults()\n",
        "    docs = tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results) # append web documents to retrieved documents\n",
        "\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer or re-generate a question for web search.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current state of the agent, including all keys.\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---DECIDE TO GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    filtered_documents = state_dict[\"documents\"]\n",
        "    search = state_dict[\"run_web_search\"]\n",
        "\n",
        "    if search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPvTea8S972F"
      },
      "source": [
        "# Build Graph\n",
        "\n",
        "This just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTWfmxxG1WTd"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search\", web_search)  # web search\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILqYhx3r-BSx"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxtAm380sqzp"
      },
      "source": [
        "## GPT4All generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i3WnsS61YmS",
        "outputId": "470cac43-7041-4ada-cfa2-9615316b786d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "-------------------- GPT4All --------------------\n",
            "-------------------- GPT4All using= True --------------------\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "[GIN] 2024/03/01 - 06:26:23 | 200 |  1.196520008s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "[GIN] 2024/03/01 - 06:26:25 | 200 |  1.511803217s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "[GIN] 2024/03/01 - 06:26:26 | 200 |  1.372822226s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "[GIN] 2024/03/01 - 06:26:27 | 200 |  643.753474ms |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---DECIDE TO GENERATE---\n",
            "---DECISION: GENERATE---\n",
            "---GENERATE---\n",
            "[GIN] 2024/03/01 - 06:26:38 | 200 |  9.875794888s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' In an LLM (large language model)-powered autonomous agent system, LLM '\n",
            " 'functions as the agent’s brain, complemented by several key components: '\n",
            " 'planning and memory.\\n'\n",
            " '\\n'\n",
            " 'Planning involves breaking down large tasks into smaller subgoals for '\n",
            " 'efficient handling of complex tasks and self-criticism and refinement to '\n",
            " 'improve results.\\n'\n",
            " '\\n'\n",
            " 'Memory includes short-term memory, which utilizes in-context learning, and '\n",
            " 'long-term memory, providing the agent with the capability to retain and '\n",
            " 'recall information over extended periods using an external vector store and '\n",
            " 'fast retrieval. The agent also learns to call external APIs for missing '\n",
            " 'information.\\n'\n",
            " '\\n'\n",
            " 'Types of Memory:\\n'\n",
            " '1. Sensory Memory: retains impressions of sensory information for a few '\n",
            " 'seconds.\\n'\n",
            " '2. Short-Term Memory (STM) or Working Memory: stores information needed for '\n",
            " 'complex cognitive tasks and lasts for 20-30 seconds.\\n'\n",
            " '3. Long-Term Memory (LTM): stores information for a remarkably long time, '\n",
            " 'with two subtypes: explicit/declarative memory and implicit/procedural '\n",
            " 'memory.')\n"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"Explain how the different types of agent memory work?\",\n",
        "        \"local\": run_local,\n",
        "        \"retriever_to_use\" : \"GPT4All\"\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2zu-rp7sn-2"
      },
      "source": [
        "## Llama.cpp generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEHzPQww1bNe",
        "outputId": "49f186d1-dcce-4566-a9da-b991e0303e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "-------------------- Llama --------------------\n",
            "-------------------- GPT4All using= False --------------------\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "[GIN] 2024/03/01 - 06:26:48 | 200 |  1.186081081s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "[GIN] 2024/03/01 - 06:26:49 | 200 |  1.523827574s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "[GIN] 2024/03/01 - 06:26:51 | 200 |  1.413219356s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "[GIN] 2024/03/01 - 06:26:51 | 200 |   646.22748ms |       127.0.0.1 | POST     \"/api/chat\"\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---DECIDE TO GENERATE---\n",
            "---DECISION: GENERATE---\n",
            "---GENERATE---\n",
            "[GIN] 2024/03/01 - 06:27:02 | 200 |  9.379334629s |       127.0.0.1 | POST     \"/api/chat\"\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' In an LLM (large language model)-powered autonomous agent system, LLM '\n",
            " 'functions as the agent’s brain, complemented by several key components: '\n",
            " 'planning and memory.\\n'\n",
            " '\\n'\n",
            " 'Planning involves breaking down large tasks into smaller subgoals for '\n",
            " 'efficient handling of complex tasks and self-criticism and refinement to '\n",
            " 'improve results.\\n'\n",
            " '\\n'\n",
            " 'Memory includes short-term memory, which utilizes in-context learning, and '\n",
            " 'long-term memory, providing the agent with the capability to retain and '\n",
            " 'recall information over extended periods using an external vector store and '\n",
            " 'fast retrieval. The agent also learns to call external APIs for missing '\n",
            " 'information.\\n'\n",
            " '\\n'\n",
            " 'Types of Memory:\\n'\n",
            " '1. Sensory Memory: retains impressions of sensory information for a few '\n",
            " 'seconds.\\n'\n",
            " '2. Short-Term Memory (STM) or Working Memory: stores information needed for '\n",
            " 'complex cognitive tasks and lasts for 20-30 seconds.\\n'\n",
            " '3. Long-Term Memory (LTM): stores information for a remarkably long time, '\n",
            " 'with two subtypes: explicit/declarative memory and implicit/procedural '\n",
            " 'memory.')\n"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"Explain how the different types of agent memory work?\",\n",
        "        \"local\": run_local,\n",
        "        \"retriever_to_use\" : \"Llama\"\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqGeEbWDsuA5"
      },
      "source": [
        "# Creating a server with google colab for local computer access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2NW4tH-8JHL"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "- aiohttp\n",
        "- pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97HPaX308a5C",
        "outputId": "856e9c15-fe95-4c17-ccd2-b9371a6459d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.3)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install aiohttp pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCOgpg7eCr2k"
      },
      "source": [
        "## NGrok access token\n",
        "\n",
        "1. Create an ngrok account from [here](https://ngrok.com/).\n",
        "\n",
        "2. Get [ngrok auth token from here](https://dashboard.ngrok.com/get-started/your-authtoken)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOf1dGDCtpV"
      },
      "outputs": [],
      "source": [
        "NGROK_ACCESS_TOKEN = '<NGROK_ACCESS_TOKEN>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U02Ztem68btt"
      },
      "source": [
        "## Start async processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDEbkLfmsygL",
        "outputId": "2001e064-7513-4aa1-fc8d-4989505ece09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> starting ngrok config add-authtoken 2d4vI2QK5xk1qcer128P4Q1E6EJ_2DKznD7bSFCsAqmGk5T9S\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434\n",
            "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n",
            "t=2024-03-01T09:54:17+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2024-03-01T09:54:17+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2024-03-01T09:54:17+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "t=2024-03-01T09:54:17+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2024-03-01T09:54:18+0000 lvl=info msg=\"client session established\" obj=tunnels.session obj=csess id=3be6455cdd9c\n",
            "t=2024-03-01T09:54:18+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2024-03-01T09:54:18+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://6ea3-35-247-48-241.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "#register an account at ngrok.com and create an authtoken and place it here\n",
        "await asyncio.gather(\n",
        "    run_process(['ngrok', 'config', 'add-authtoken', NGROK_ACCESS_TOKEN])\n",
        ")\n",
        "\n",
        "await asyncio.gather(\n",
        "    run_process(['ollama', 'serve']),\n",
        "    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kiy_YNG8IYey"
      },
      "source": [
        "## Connecting from Local PC\n",
        "\n",
        "The below code provided is for connecting from your local device via ngrok exposed url.\n",
        "\n",
        "\n",
        "- Once the ngrok service has been started then copy the below code for connecting with google colab using local device\n",
        "\n",
        "```python\n",
        "#! Python 3.8.10\n",
        "import requests\n",
        "import json\n",
        "import argparse\n",
        "\n",
        "\n",
        "# Example URL to make a GET request\n",
        "NGROK_EXPOSED_URL = \"<NGROK_EXPOSED_URL>\"\n",
        "OLLAMA_URL = \"http://127.0.0.1:11434/api/generate\"\n",
        "LOCAL_LLM = \"mistral:instruct\"\n",
        "\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Process some integers.\")\n",
        "\n",
        "    # Define command-line arguments with default values\n",
        "    parser.add_argument(\n",
        "        \"--model\", type=str, default=\"mistral:instruct\", help=\"model name eg. mistral\"\n",
        "    )\n",
        "    parser.add_argument(\"--url\", type=str, help=\"url of exposed api\")\n",
        "    parser.add_argument(\n",
        "        \"--forever\", action=\"store_true\", help=\"run only one time or in loop\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--stream\", action=\"store_false\", help=\"output from url will be streamed\"\n",
        "    )\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Convert parsed arguments into a dictionary\n",
        "    args_dict = vars(args)\n",
        "\n",
        "    # Filter out arguments with default values\n",
        "    args_dict = {k: v for k, v in args_dict.items() if v is not None}\n",
        "\n",
        "    return args_dict\n",
        "\n",
        "\n",
        "def ollama_generate_from_url(\n",
        "    user_query: str, url: str = OLLAMA_URL, model=LOCAL_LLM, stream=True\n",
        "):\n",
        "    print(\"*\" * 30 + \" Configs \" + \"*\" * 30)\n",
        "    print(f\"{user_query = }\")\n",
        "    print(f\"{url = }\")\n",
        "    print(f\"{model = }\")\n",
        "    print(f\"{stream = }\")\n",
        "    print(\"*\" * 30 + \"  \" + \"*\" * 30)\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model,  # Your model name\n",
        "        \"prompt\": user_query,  # prompt for the model\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Making a GET request with stream=True\n",
        "        response = requests.post(url, json=payload, stream=stream)\n",
        "\n",
        "        # Iterating over the response content line by line\n",
        "        for line in response.iter_lines():\n",
        "            if line:\n",
        "                # Process each line of the streaming response\n",
        "                response_dict = json.loads(line.decode(\"utf-8\"))\n",
        "                print(response_dict[\"response\"], end=\"\")  # Decode bytes to string\n",
        "    except requests.RequestException as e:\n",
        "        print(\"Error occurred:\", e)\n",
        "\n",
        "\n",
        "def start_ollama_server(\n",
        "    model: str = LOCAL_LLM, forever=False, url: str = OLLAMA_URL, stream=True\n",
        "):\n",
        "\n",
        "    user_query = input(f\"\\nAsk ollama: \")\n",
        "    ollama_generate_from_url(model=model, url=url, user_query=user_query, stream=stream)\n",
        "\n",
        "    while forever:\n",
        "        user_query = input(f\"Ask ollama: \")\n",
        "        ollama_generate_from_url(\n",
        "            model=model, url=url, user_query=user_query, stream=stream\n",
        "        )\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_arguments()\n",
        "    start_ollama_server(**args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "- Save this code as `ollama-colab.py`.\n",
        "\n",
        "- From terminal __run the below command__\n",
        "\n",
        "    `python ollama-colab.py --url <ngrok-exposed-url>`\n",
        "\n",
        "- Running the ollama locally can also be done with this script. If the url is not provided it will try to access local device localhost for generation at the port 11434. Run the below command for local ollama access\n",
        "\n",
        "    ```python ollama-colab.py```\n",
        "    or\n",
        "    ```python ollama-colab.py --url http://127.0.0.1:11434/api/generate```\n",
        "\n",
        "\n",
        "- __Other optional args__.\n",
        "\n",
        "    - model (str): The default model will be `mistral:instruct`. Provide any other model name to generate response.\n",
        "\n",
        "    ```python ollama-colab.py --model gemma```\n",
        "\n",
        "    - stream (boolean): This will use the streaming for the post request on ollama host.\n",
        "\n",
        "    ```python ollama-colab.py --stream```\n",
        "\n",
        "    - forever (boolean): This will run an infinite while loop for chat like feel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tavmw9Krjiwi"
      },
      "source": [
        "# References:\n",
        "\n",
        "- Ollama api docs: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
        "- Langchain corrective rag: https://www.youtube.com/watch?v=E2shqsYwxck"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IIxkxMSD88kw",
        "69B1GLSL9O4B",
        "bWtNAMHv9pyJ",
        "C_3NvSPs93u9",
        "gPvTea8S972F",
        "ILqYhx3r-BSx"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}